# ====================================================================================
# Final Production Image (FLUX S3 Version with Network Volume)
# This Dockerfile creates a lean image that expects models to be on a network volume.
# ====================================================================================
FROM nvidia/cuda:12.1.1-base-ubuntu22.04

WORKDIR /app

# Install system dependencies, including git for the diffusers dependency
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    curl \
    ca-certificates \
    git \
    && rm -rf /var/lib/apt/lists/*

# Install uv, the Rust-based Python package manager
RUN curl -LsSf https://astral.sh/uv/install.sh | sh
ENV PATH="/root/.local/bin:$PATH"

# Set up cache directories for uv
ENV UV_CACHE_DIR=/opt/uv/cache
ENV UV_PYTHON_INSTALL_DIR=/opt/python

# Install a specific Python version
RUN uv python install 3.9

# Create and activate a virtual environment
ENV VENV_PATH=/opt/venv
ENV UV_PROJECT_ENVIRONMENT=$VENV_PATH
RUN uv venv $VENV_PATH --python 3.9
ENV PATH="$VENV_PATH/bin:$PATH"

# Copy only the requirements file first to leverage Docker layer caching
COPY ./CatVTON/requirements.txt /app/requirements.txt

# Install all Python dependencies in a single, consolidated step
# This includes torch, s3 dependencies, and the project requirements
RUN uv pip install \
    --index-url https://download.pytorch.org/whl/cu121 \
    torch torchvision torchaudio \
    boto3 botocore \
    -r /app/requirements.txt

# Now copy the rest of the application code
COPY ./CatVTON /app/CatVTON
COPY app_flux_s3.py ./app.py

# Models are NOT copied. They will be loaded from the /runpod-volume mount.
# Set the HF_HOME to the network volume path so transformers can find models there.
ENV HF_HOME=/runpod-volume/models
ENV HUGGINGFACE_HUB_CACHE=/runpod-volume/models

EXPOSE 8000
CMD ["python", "-u", "app.py"]